; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=xtensa -mcpu=cnl %s -o - | FileCheck %s

define i32 @test_2xi32toi32(<2 x i32> %a)  {
; CHECK-LABEL: test_2xi32toi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a2, aed0
; CHECK-NEXT:    retw.n
  %r = extractelement <2 x i32> %a, i32 0
  ret i32 %r
}

define <2 x i32> @test_i32to2xi32(i32 %a)  {
; CHECK-LABEL: test_i32to2xi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movda32x2 aed0, a2, a2
; CHECK-NEXT:    retw.n
  %vecinit = insertelement <2 x i32> undef, i32 %a, i64 0
  %vecinit1 = shufflevector <2 x i32> %vecinit, <2 x i32> poison, <2 x i32> zeroinitializer
  ret <2 x i32> %vecinit1
}

define void @test_store_2xi32(i32 %a, <2 x i32> %v) {
; CHECK-LABEL: test_store_2xi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_s32x2.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  store <2 x i32> %v, ptr %p, align 8
  ret void
}

define void @test_store_1xi64(i32 %a, <1 x i64> %v) {
; CHECK-LABEL: test_store_1xi64:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_s64.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  store <1 x i64> %v, ptr %p, align 8
  ret void
}

define <1 x i64> @test_build_1xi64(i64 %v) {
; CHECK-LABEL: test_build_1xi64:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movda32x2 aed0, a3, a2
; CHECK-NEXT:    retw.n
  %vec = insertelement <1 x i64> undef, i64 %v, i64 0
  ret <1 x i64> %vec
}

define void @test_store_4xi16(i32 %a, <4 x i16> %v) {
; CHECK-LABEL: test_store_4xi16:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_s16x4.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  store <4 x i16> %v, ptr %p, align 8
  ret void
}

define <2 x i32> @test_load_2xi32(i32 %a) {
; CHECK-LABEL: test_load_2xi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_l32x2.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  %v = load <2 x i32>, ptr %p, align 8
  ret <2 x i32> %v
}

define <1 x i64> @test_load_1xi64(i32 %a) {
; CHECK-LABEL: test_load_1xi64:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_l64.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  %v = load <1 x i64>, ptr %p, align 8
  ret <1 x i64> %v
}

define <4 x i16> @test_load_4xi16(i32 %a) {
; CHECK-LABEL: test_load_4xi16:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_l16x4.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  %v = load <4 x i16>, ptr %p, align 8
  ret <4 x i16> %v
}

define void @test_build_store_1xi32(i32 %a, i32 %v) {
; CHECK-LABEL: test_build_store_1xi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movda32 aed0, a3
; CHECK-NEXT:    ae_s32.l.i aed0, a2, 0
; CHECK-NEXT:    retw.n
  %vec = insertelement <1 x i32> undef, i32 %v, i64 0
  %p = inttoptr i32 %a to ptr
  store <1 x i32> %vec, ptr %p, align 8
  ret void
}

define i32 @test_load_extract_1xi32(i32 %a) {
; CHECK-LABEL: test_load_extract_1xi32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    l32i.n a2, a2, 0
; CHECK-NEXT:    retw.n
  %p = inttoptr i32 %a to ptr
  %vec = load <1 x i32>, ptr %p, align 8
  %r = extractelement <1 x i32> %vec, i32 0
  ret i32 %r
}

define <4 x i16> @test_build_4xi16_2(i16 %a, i16 %b) {
; CHECK-LABEL: test_build_4xi16_2:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movda16x2 aed0, a2, a3
; CHECK-NEXT:    retw.n
  %vecinit = insertelement <4 x i16> undef, i16 %a, i64 0
  %vecinit1 = insertelement <4 x i16> %vecinit, i16 %b, i64 1
  %vecinit2 = insertelement <4 x i16> %vecinit1, i16 %a, i64 2
  %vecinit3 = insertelement <4 x i16> %vecinit2, i16 %b, i64 3
  ret <4 x i16> %vecinit3
}

define <4 x i16> @test_build_4xi16_1(i16 %a) {
; CHECK-LABEL: test_build_4xi16_1:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movda16 aed0, a2
; CHECK-NEXT:    retw.n
  %vecinit = insertelement <4 x i16> undef, i16 %a, i64 0
  %vecinit1 = shufflevector <4 x i16> %vecinit, <4 x i16> poison, <4 x i32> zeroinitializer
  ret <4 x i16> %vecinit1
}

define i32 @test_extract(<2 x i32> %v2i, <1 x i32> %v1i, <4 x i16> %v4s, <1 x i64> %v1l) {
; CHECK-LABEL: test_extract:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad16.0 a8, aed2
; CHECK-NEXT:    l32r a9, .LCPI13_0
; CHECK-NEXT:    and a8, a8, a9
; CHECK-NEXT:    ae_movad32.h a10, aed0
; CHECK-NEXT:    ae_movad32.l a11, aed0
; CHECK-NEXT:    add.n a10, a11, a10
; CHECK-NEXT:    ae_movad32.l a11, aed1
; CHECK-NEXT:    add.n a10, a10, a11
; CHECK-NEXT:    add.n a8, a8, a10
; CHECK-NEXT:    ae_movad16.1 a10, aed2
; CHECK-NEXT:    and a10, a10, a9
; CHECK-NEXT:    add.n a8, a10, a8
; CHECK-NEXT:    ae_movad16.2 a10, aed2
; CHECK-NEXT:    and a10, a10, a9
; CHECK-NEXT:    add.n a8, a10, a8
; CHECK-NEXT:    ae_movad16.3 a10, aed2
; CHECK-NEXT:    and a9, a10, a9
; CHECK-NEXT:    add.n a8, a9, a8
; CHECK-NEXT:    ae_movad32.l a9, aed3
; CHECK-NEXT:    add.n a2, a9, a8
; CHECK-NEXT:    retw.n
  %v2i0 = extractelement <2 x i32> %v2i, i64 0
  %v2i1 = extractelement <2 x i32> %v2i, i64 1
  %sum1 = add i32 %v2i0, %v2i1
  %v1i0 = extractelement <1 x i32> %v1i, i64 0
  %sum2 = add i32 %sum1, %v1i0
  %v4s0 = extractelement <4 x i16> %v4s, i64 0
  %v4s0i = zext i16 %v4s0 to i32
  %sum3 = add i32 %v4s0i, %sum2
  %v4s1 = extractelement <4 x i16> %v4s, i64 1
  %v4s1i = zext i16 %v4s1 to i32
  %sum4 = add i32 %v4s1i, %sum3
  %v4s2 = extractelement <4 x i16> %v4s, i64 2
  %v4s2i = zext i16 %v4s2 to i32
  %sum5 = add i32 %v4s2i, %sum4
  %v4s3 = extractelement <4 x i16> %v4s, i64 3
  %v4s3i = zext i16 %v4s3 to i32
  %sum6 = add i32 %v4s3i, %sum5
  %v1l0 = extractelement <1 x i64> %v1l, i64 0
  %v1l0l = trunc i64 %v1l0 to i32
  %sum7 = add i32 %v1l0l, %sum6

  ret i32 %sum7
}

define <1 x i32> @test_extract_subvec_1x32(<2 x i32> %v) {
; CHECK-LABEL: test_extract_subvec_1x32:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ae_movda32 aed0, a8
; CHECK-NEXT:    retw.n
  %shuffle = shufflevector <2 x i32> %v, <2 x i32> poison, <1 x i32> zeroinitializer
  ret <1 x i32> %shuffle
}


define <4 x i16> @rlshift4(<4 x i16> %a, i16 signext %b) {
; CHECK-LABEL: rlshift4:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    l32r a8, .LCPI15_0
; CHECK-NEXT:    and a9, a2, a8
; CHECK-NEXT:    ae_movad16.3 a10, aed0
; CHECK-NEXT:    and a10, a10, a8
; CHECK-NEXT:    ssr a9
; CHECK-NEXT:    srl a10, a10
; CHECK-NEXT:    ae_movad16.2 a11, aed0
; CHECK-NEXT:    and a11, a11, a8
; CHECK-NEXT:    ssr a9
; CHECK-NEXT:    srl a11, a11
; CHECK-NEXT:    ae_movda16x2 aed1, a11, a10
; CHECK-NEXT:    ae_movad16.1 a10, aed0
; CHECK-NEXT:    and a10, a10, a8
; CHECK-NEXT:    ssr a9
; CHECK-NEXT:    srl a10, a10
; CHECK-NEXT:    ae_movad16.0 a11, aed0
; CHECK-NEXT:    and a8, a11, a8
; CHECK-NEXT:    ssr a9
; CHECK-NEXT:    srl a8, a8
; CHECK-NEXT:    ae_movda16x2 aed0, a8, a10
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %v = insertelement <4 x i16> undef, i16 %b, i64 0
  %sh_prom = shufflevector <4 x i16> %v, <4 x i16> poison, <4 x i32> zeroinitializer
  %shr = lshr <4 x i16> %a, %sh_prom
  ret <4 x i16> %shr
}


define <4 x i16> @rlshift4_imm(<4 x i16> %a) {
; CHECK-LABEL: rlshift4_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad16.3 a8, aed0
; CHECK-NEXT:    l32r a9, .LCPI16_0
; CHECK-NEXT:    and a8, a8, a9
; CHECK-NEXT:    srli a8, a8, 1
; CHECK-NEXT:    ae_movad16.2 a10, aed0
; CHECK-NEXT:    and a10, a10, a9
; CHECK-NEXT:    srli a10, a10, 1
; CHECK-NEXT:    ae_movda16x2 aed1, a10, a8
; CHECK-NEXT:    ae_movad16.1 a8, aed0
; CHECK-NEXT:    and a8, a8, a9
; CHECK-NEXT:    srli a8, a8, 1
; CHECK-NEXT:    ae_movad16.0 a10, aed0
; CHECK-NEXT:    and a9, a10, a9
; CHECK-NEXT:    srli a9, a9, 1
; CHECK-NEXT:    ae_movda16x2 aed0, a9, a8
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %shr = lshr <4 x i16> %a, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}


define <2 x i32> @rlshift2(<2 x i32> %a, i32 %b) {
; CHECK-LABEL: rlshift2:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    srl a8, a8
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    srl a9, a9
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %splat.splatinsert = insertelement <2 x i32> poison, i32 %b, i64 0
  %splat.splat = shufflevector <2 x i32> %splat.splatinsert, <2 x i32> poison, <2 x i32> zeroinitializer
  %shr = lshr <2 x i32> %a, %splat.splat
  ret <2 x i32> %shr
}


define <2 x i32> @rlshift2_imm(<2 x i32> %a) {
; CHECK-LABEL: rlshift2_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    srli a8, a8, 1
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    srli a9, a9, 1
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %shr = lshr <2 x i32> %a, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <1 x i64> @rlshift1(<1 x i64> %a, i32 %b) {
; CHECK-LABEL: rlshift1:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ae_movda32 aed0, a2
; CHECK-NEXT:    ae_srli64 aed0, aed0, 32
; CHECK-NEXT:    ae_movad32.l a10, aed0
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    src a8, a9, a8
; CHECK-NEXT:    addi a11, a10, -32
; CHECK-NEXT:    ssr a11
; CHECK-NEXT:    srl a13, a9
; CHECK-NEXT:    movi.n a12, 0
; CHECK-NEXT:    blt a11, a12, .LBB19_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a8, a13
; CHECK-NEXT:  .LBB19_2:
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    srl a9, a9
; CHECK-NEXT:    blt a11, a12, .LBB19_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    mov.n a9, a12
; CHECK-NEXT:  .LBB19_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
   %splat.splatinsert = insertelement <1 x i32> poison, i32 %b, i64 0
  %sh_prom = zext <1 x i32> %splat.splatinsert to <1 x i64>
  %shr = lshr <1 x i64> %a, %sh_prom
  ret <1 x i64> %shr
}

define <1 x i64> @rlshift1_imm(<1 x i64> %a) {
; CHECK-LABEL: rlshift1_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ae_movi aed0, 1
; CHECK-NEXT:    ae_srli64 aed0, aed0, 32
; CHECK-NEXT:    ae_movad32.l a10, aed0
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    src a8, a9, a8
; CHECK-NEXT:    addi a11, a10, -32
; CHECK-NEXT:    ssr a11
; CHECK-NEXT:    srl a13, a9
; CHECK-NEXT:    movi.n a12, 0
; CHECK-NEXT:    blt a11, a12, .LBB20_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a8, a13
; CHECK-NEXT:  .LBB20_2:
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    srl a9, a9
; CHECK-NEXT:    blt a11, a12, .LBB20_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    mov.n a9, a12
; CHECK-NEXT:  .LBB20_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %shr = lshr <1 x i64> %a, <i64 1>
  ret <1 x i64> %shr
}

define <4 x i16> @rashift4(<4 x i16> %a, i16 signext %b) {
; CHECK-LABEL: rashift4:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    l32r a8, .LCPI21_0
; CHECK-NEXT:    and a8, a2, a8
; CHECK-NEXT:    ae_movad16.3 a9, aed0
; CHECK-NEXT:    sext a9, a9, 15
; CHECK-NEXT:    ssr a8
; CHECK-NEXT:    sra a9, a9
; CHECK-NEXT:    ae_movad16.2 a10, aed0
; CHECK-NEXT:    sext a10, a10, 15
; CHECK-NEXT:    ssr a8
; CHECK-NEXT:    sra a10, a10
; CHECK-NEXT:    ae_movda16x2 aed1, a10, a9
; CHECK-NEXT:    ae_movad16.1 a9, aed0
; CHECK-NEXT:    sext a9, a9, 15
; CHECK-NEXT:    ssr a8
; CHECK-NEXT:    sra a9, a9
; CHECK-NEXT:    ae_movad16.0 a10, aed0
; CHECK-NEXT:    sext a10, a10, 15
; CHECK-NEXT:    ssr a8
; CHECK-NEXT:    sra a8, a10
; CHECK-NEXT:    ae_movda16x2 aed0, a8, a9
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %v = insertelement <4 x i16> undef, i16 %b, i64 0
  %sh_prom = shufflevector <4 x i16> %v, <4 x i16> poison, <4 x i32> zeroinitializer
  %shr = ashr <4 x i16> %a, %sh_prom
  ret <4 x i16> %shr
}

define <4 x i16> @rashift4_imm(<4 x i16> %a) {
; CHECK-LABEL: rashift4_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad16.3 a8, aed0
; CHECK-NEXT:    sext a8, a8, 15
; CHECK-NEXT:    srai a8, a8, 1
; CHECK-NEXT:    ae_movad16.2 a9, aed0
; CHECK-NEXT:    sext a9, a9, 15
; CHECK-NEXT:    srai a9, a9, 1
; CHECK-NEXT:    ae_movda16x2 aed1, a9, a8
; CHECK-NEXT:    ae_movad16.1 a8, aed0
; CHECK-NEXT:    sext a8, a8, 15
; CHECK-NEXT:    srai a8, a8, 1
; CHECK-NEXT:    ae_movad16.0 a9, aed0
; CHECK-NEXT:    sext a9, a9, 15
; CHECK-NEXT:    srai a9, a9, 1
; CHECK-NEXT:    ae_movda16x2 aed0, a9, a8
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %shr = ashr <4 x i16> %a, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <2 x i32> @rashift2(<2 x i32> %a, i32 %b) {
; CHECK-LABEL: rashift2:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    sra a8, a8
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    sra a9, a9
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %splat.splatinsert = insertelement <2 x i32> poison, i32 %b, i64 0
  %splat.splat = shufflevector <2 x i32> %splat.splatinsert, <2 x i32> poison, <2 x i32> zeroinitializer
  %shr = ashr <2 x i32> %a, %splat.splat
  ret <2 x i32> %shr
}


define <2 x i32> @rashift2_imm(<2 x i32> %a) {
; CHECK-LABEL: rashift2_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    srai a8, a8, 1
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    srai a9, a9, 1
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %shr = ashr <2 x i32> %a, <i32 1, i32 1>
  ret <2 x i32> %shr
}


define <1 x i64> @rashift1(<1 x i64> %a, i64 %b) {
; CHECK-LABEL: rashift1:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    src a8, a9, a8
; CHECK-NEXT:    addi a10, a2, -32
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    sra a12, a9
; CHECK-NEXT:    movi.n a11, 0
; CHECK-NEXT:    blt a10, a11, .LBB25_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a8, a12
; CHECK-NEXT:  .LBB25_2:
; CHECK-NEXT:    ssr a2
; CHECK-NEXT:    sra a12, a9
; CHECK-NEXT:    blt a10, a11, .LBB25_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    srai a12, a9, 31
; CHECK-NEXT:  .LBB25_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a12, a8
; CHECK-NEXT:    retw.n
  %splat.splatinsert = insertelement <1 x i64> poison, i64 %b, i64 0
  %shr = ashr <1 x i64> %a, %splat.splatinsert
  ret <1 x i64> %shr
}


define <1 x i64> @rashift1_imm(<1 x i64> %a) {
; CHECK-LABEL: rashift1_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a9, aed0
; CHECK-NEXT:    ae_movad32.h a8, aed0
; CHECK-NEXT:    ae_movi aed0, 1
; CHECK-NEXT:    ae_srli64 aed0, aed0, 32
; CHECK-NEXT:    ae_movad32.l a10, aed0
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    src a9, a8, a9
; CHECK-NEXT:    addi a11, a10, -32
; CHECK-NEXT:    ssr a11
; CHECK-NEXT:    sra a13, a8
; CHECK-NEXT:    movi.n a12, 0
; CHECK-NEXT:    blt a11, a12, .LBB26_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a9, a13
; CHECK-NEXT:  .LBB26_2:
; CHECK-NEXT:    ssr a10
; CHECK-NEXT:    sra a10, a8
; CHECK-NEXT:    blt a11, a12, .LBB26_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    srai a10, a8, 31
; CHECK-NEXT:  .LBB26_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a10, a9
; CHECK-NEXT:    retw.n
  %shr = ashr <1 x i64> %a, <i64 1>
  ret <1 x i64> %shr
}


define <4 x i16> @lshift4(<4 x i16> %a, i16 signext %b) {
; CHECK-LABEL: lshift4:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    l32r a8, .LCPI27_0
; CHECK-NEXT:    and a8, a2, a8
; CHECK-NEXT:    ae_movad16.3 a9, aed0
; CHECK-NEXT:    ssl a8
; CHECK-NEXT:    sll a9, a9
; CHECK-NEXT:    ae_movad16.2 a10, aed0
; CHECK-NEXT:    ssl a8
; CHECK-NEXT:    sll a10, a10
; CHECK-NEXT:    ae_movda16x2 aed1, a10, a9
; CHECK-NEXT:    ae_movad16.1 a9, aed0
; CHECK-NEXT:    ssl a8
; CHECK-NEXT:    sll a9, a9
; CHECK-NEXT:    ae_movad16.0 a10, aed0
; CHECK-NEXT:    ssl a8
; CHECK-NEXT:    sll a8, a10
; CHECK-NEXT:    ae_movda16x2 aed0, a8, a9
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %v = insertelement <4 x i16> undef, i16 %b, i64 0
  %sh_prom = shufflevector <4 x i16> %v, <4 x i16> poison, <4 x i32> zeroinitializer
  %shl = shl <4 x i16> %a, %sh_prom
  ret <4 x i16> %shl
}


define <4 x i16> @lshift4_imm(<4 x i16> %a) {
; CHECK-LABEL: lshift4_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad16.3 a8, aed0
; CHECK-NEXT:    _slli a8, a8, 1
; CHECK-NEXT:    ae_movad16.2 a9, aed0
; CHECK-NEXT:    _slli a9, a9, 1
; CHECK-NEXT:    ae_movda16x2 aed1, a9, a8
; CHECK-NEXT:    ae_movad16.1 a8, aed0
; CHECK-NEXT:    _slli a8, a8, 1
; CHECK-NEXT:    ae_movad16.0 a9, aed0
; CHECK-NEXT:    _slli a9, a9, 1
; CHECK-NEXT:    ae_movda16x2 aed0, a9, a8
; CHECK-NEXT:    ae_slai64 aed0, aed0, 32
; CHECK-NEXT:    ae_or aed0, aed0, aed1
; CHECK-NEXT:    retw.n
  %shl = shl <4 x i16> %a, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shl
}


define <2 x i32> @lshift2(<2 x i32> %a, i32 %b) {
; CHECK-LABEL: lshift2:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ssl a2
; CHECK-NEXT:    sll a8, a8
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ssl a2
; CHECK-NEXT:    sll a9, a9
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %splat.splatinsert = insertelement <2 x i32> poison, i32 %b, i64 0
  %splat.splat = shufflevector <2 x i32> %splat.splatinsert, <2 x i32> poison, <2 x i32> zeroinitializer
  %shl = shl <2 x i32> %a, %splat.splat
  ret <2 x i32> %shl
}


define <2 x i32> @lshift2_imm(<2 x i32> %a) {
; CHECK-LABEL: lshift2_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    _slli a8, a8, 1
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    _slli a9, a9, 1
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %shl = shl <2 x i32> %a, <i32 1, i32 1>
  ret <2 x i32> %shl
}


define <1 x i64> @lshift1(<1 x i64> %a, i64 %b) {
; CHECK-LABEL: lshift1:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a9, aed0
; CHECK-NEXT:    ae_movad32.h a8, aed0
; CHECK-NEXT:    ssl a2
; CHECK-NEXT:    src a8, a8, a9
; CHECK-NEXT:    addi a10, a2, -32
; CHECK-NEXT:    ssl a10
; CHECK-NEXT:    sll a12, a9
; CHECK-NEXT:    movi.n a11, 0
; CHECK-NEXT:    blt a10, a11, .LBB31_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a8, a12
; CHECK-NEXT:  .LBB31_2:
; CHECK-NEXT:    ssl a2
; CHECK-NEXT:    sll a9, a9
; CHECK-NEXT:    blt a10, a11, .LBB31_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    mov.n a9, a11
; CHECK-NEXT:  .LBB31_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a8, a9
; CHECK-NEXT:    retw.n
  %splat.splatinsert = insertelement <1 x i64> poison, i64 %b, i64 0
  %shl = shl <1 x i64> %a, %splat.splatinsert
  ret <1 x i64> %shl
}


define <1 x i64> @lshift1_imm(<1 x i64> %a) {
; CHECK-LABEL: lshift1_imm:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_movad32.l a8, aed0
; CHECK-NEXT:    ae_movad32.h a9, aed0
; CHECK-NEXT:    ae_movi aed0, 1
; CHECK-NEXT:    ae_srli64 aed0, aed0, 32
; CHECK-NEXT:    ae_movad32.l a10, aed0
; CHECK-NEXT:    ssl a10
; CHECK-NEXT:    src a9, a9, a8
; CHECK-NEXT:    addi a11, a10, -32
; CHECK-NEXT:    ssl a11
; CHECK-NEXT:    sll a13, a8
; CHECK-NEXT:    movi.n a12, 0
; CHECK-NEXT:    blt a11, a12, .LBB32_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mov.n a9, a13
; CHECK-NEXT:  .LBB32_2:
; CHECK-NEXT:    ssl a10
; CHECK-NEXT:    sll a8, a8
; CHECK-NEXT:    blt a11, a12, .LBB32_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    mov.n a8, a12
; CHECK-NEXT:  .LBB32_4:
; CHECK-NEXT:    ae_movda32x2 aed0, a9, a8
; CHECK-NEXT:    retw.n
  %shl = shl <1 x i64> %a, <i64 1>
  ret <1 x i64> %shl
}

define void @test_valign_load_store(i32 %p1, i32 %p2) {
; CHECK-LABEL: test_valign_load_store:
; CHECK:         .cfi_startproc
; CHECK-NEXT:  # %bb.0:
; CHECK-NEXT:    entry a1, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ae_lalign64.i u0, a2, 0
; CHECK-NEXT:    ae_salign64.i u0, a3, 0
; CHECK-NEXT:    retw.n
  %ptr1 = inttoptr i32 %p1 to ptr
  %ptr2 = inttoptr i32 %p2 to ptr
  %v = load <8 x i8>, ptr %ptr1, align 8
  store <8 x i8> %v, ptr %ptr2, align 8
  ret void
}
