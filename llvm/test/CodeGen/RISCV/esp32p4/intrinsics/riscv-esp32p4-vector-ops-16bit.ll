; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; ModuleID = './riscv-esp32p4-vector-ops-16bit.c'
; Test ASM generation (Intrinsic -> ASM)
; RUN: llc -O2 -mattr=xespv2p1 -mtriple=riscv32 %s -o - | FileCheck %s --check-prefix=ASM

define dso_local void @test_vadd_s16(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_vadd_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.vadd.s16 q0, q0, q1
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %4 = extractvalue { <16 x i8>, ptr } %3, 0
  %5 = bitcast <16 x i8> %4 to <8 x i16>
  %6 = tail call <8 x i16> @llvm.riscv.esp.vadd.s16.m(<8 x i16> %2, <8 x i16> %5)
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %7, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.vadd.s16.m(<8 x i16>, <8 x i16>) #1

define dso_local void @test_vadd_u16(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_vadd_u16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.vadd.u16 q0, q0, q1
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %4 = extractvalue { <16 x i8>, ptr } %3, 0
  %5 = bitcast <16 x i8> %4 to <8 x i16>
  %6 = tail call <8 x i16> @llvm.riscv.esp.vadd.u16.m(<8 x i16> %2, <8 x i16> %5)
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %7, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.vadd.u16.m(<8 x i16>, <8 x i16>) #1

define dso_local void @test_vsub_s16(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_vsub_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.vsub.s16 q0, q0, q1
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %4 = extractvalue { <16 x i8>, ptr } %3, 0
  %5 = bitcast <16 x i8> %4 to <8 x i16>
  %6 = tail call <8 x i16> @llvm.riscv.esp.vsub.s16.m(<8 x i16> %2, <8 x i16> %5)
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %7, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.vsub.s16.m(<8 x i16>, <8 x i16>) #1

define dso_local void @test_vmul_s16(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst, i32 noundef %sar_val) local_unnamed_addr #0 {
; ASM-LABEL: test_vmul_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.movx.w.sar a3
; ASM-NEXT:    esp.vmul.s16 q0, q0, q1
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %4 = extractvalue { <16 x i8>, ptr } %3, 0
  %5 = bitcast <16 x i8> %4 to <8 x i16>
  %6 = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 %sar_val)
  %7 = tail call <8 x i16> @llvm.riscv.esp.vmul.s16.m(<8 x i16> %2, <8 x i16> %5, i32 %6)
  %8 = bitcast <8 x i16> %7 to <16 x i8>
  %9 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %8, ptr %dst, i32 16)
  ret void
}

declare i32 @llvm.riscv.esp.movx.w.sar.m(i32) #1

declare <8 x i16> @llvm.riscv.esp.vmul.s16.m(<8 x i16>, <8 x i16>, i32) #1

define dso_local void @test_vmax_s16(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_vmax_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.vmax.s16 q0, q0, q1
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %4 = extractvalue { <16 x i8>, ptr } %3, 0
  %5 = bitcast <16 x i8> %4 to <8 x i16>
  %6 = tail call <8 x i16> @llvm.riscv.esp.vmax.s16.m(<8 x i16> %2, <8 x i16> %5)
  %7 = bitcast <8 x i16> %6 to <16 x i8>
  %8 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %7, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.vmax.s16.m(<8 x i16>, <8 x i16>) #1

define dso_local void @test_vabs_16(ptr noundef %src, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_vabs_16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vabs.16 q0, q0
; ASM-NEXT:    esp.vst.128.ip q0, a1, 16
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call <8 x i16> @llvm.riscv.esp.vabs.16.m(<8 x i16> %2)
  %4 = bitcast <8 x i16> %3 to <16 x i8>
  %5 = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %4, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.vabs.16.m(<8 x i16>) #1

define dso_local i32 @test_max_s16(ptr noundef %src) local_unnamed_addr #2 {
; ASM-LABEL: test_max_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.max.s16.a q0, a0
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call i32 @llvm.riscv.esp.max.s16.a.m(<8 x i16> %2)
  ret i32 %3
}

declare i32 @llvm.riscv.esp.max.s16.a.m(<8 x i16>) #1

define dso_local i32 @test_min_s16(ptr noundef %src) local_unnamed_addr #2 {
; ASM-LABEL: test_min_s16:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.min.s16.a q0, a0
; ASM-NEXT:    ret
entry:
  %0 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src, i32 16)
  %1 = extractvalue { <16 x i8>, ptr } %0, 0
  %2 = bitcast <16 x i8> %1 to <8 x i16>
  %3 = tail call i32 @llvm.riscv.esp.min.s16.a.m(<8 x i16> %2)
  ret i32 %3
}

declare i32 @llvm.riscv.esp.min.s16.a.m(<8 x i16>) #1

declare ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8>, ptr, i32) #3

declare { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr, i32) #4

attributes #0 = { "target-features"="+32bit,+xespv" }
attributes #1 = { nounwind }
attributes #2 = { "target-features"="+32bit,+xespv" }
attributes #3 = { nounwind }
attributes #4 = { nounwind }


