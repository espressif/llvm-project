; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; ModuleID = './riscv-esp32p4-cmul.c'
; Test ASM generation (Intrinsic -> ASM)
; RUN: llc -O2 -mattr=xespv2p1 -mtriple=riscv32 %s -o - | FileCheck %s --check-prefix=ASM

define dso_local void @test_cmul_u16_ld_incp_with_wrapper(ptr noundef %src1, ptr noundef %src2, ptr noundef readnone captures(none) %src3, ptr noundef readnone captures(none) %src4, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_u16_ld_incp_with_wrapper:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    mv a2, a0
; ASM-NEXT:    esp.vld.128.ip q0, a2, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    esp.movx.w.sar a1
; ASM-NEXT:    esp.cmul.u16.ld.incp q1, a0, q0, q0, q1, 3
; ASM-NEXT:    esp.vst.128.ip q0, a4, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %cmul_ld_res = tail call { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u16.ld.incp.m(<8 x i16> undef, <8 x i16> %bc1, <8 x i16> %bc2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <8 x i16>, <16 x i8>, ptr } %cmul_ld_res, 0
  %bc3 = bitcast <8 x i16> %ev3 to <16 x i8>
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %bc3, ptr %dst, i32 16)
  ret void
}

declare i32 @llvm.riscv.esp.movx.w.sar.m(i32) #1

declare { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u16.ld.incp.m(<8 x i16>, <8 x i16>, <8 x i16>, ptr, i32, i32) #2

define dso_local void @test_cmul_s16_ld_incp_with_wrapper(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s16_ld_incp_with_wrapper:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    mv a3, a0
; ASM-NEXT:    esp.vld.128.ip q0, a3, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    esp.movx.w.sar a1
; ASM-NEXT:    esp.cmul.s16.ld.incp q1, a0, q0, q0, q1, 3
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %cmul_ld_res = tail call { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s16.ld.incp.m(<8 x i16> undef, <8 x i16> %bc1, <8 x i16> %bc2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <8 x i16>, <16 x i8>, ptr } %cmul_ld_res, 0
  %bc3 = bitcast <8 x i16> %ev3 to <16 x i8>
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %bc3, ptr %dst, i32 16)
  ret void
}

declare { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s16.ld.incp.m(<8 x i16>, <8 x i16>, <8 x i16>, ptr, i32, i32) #2

define dso_local void @test_cmul_u8_ld_incp_with_wrapper(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_u8_ld_incp_with_wrapper:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    mv a3, a0
; ASM-NEXT:    esp.vld.128.ip q0, a3, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    esp.movx.w.sar a1
; ASM-NEXT:    esp.cmul.u8.ld.incp q1, a0, q0, q0, q1, 3
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %cmul_ld_res = tail call { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u8.ld.incp.m(<16 x i8> undef, <16 x i8> %ev1, <16 x i8> %ev2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <16 x i8>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %ev3, ptr %dst, i32 16)
  ret void
}

declare { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u8.ld.incp.m(<16 x i8>, <16 x i8>, <16 x i8>, ptr, i32, i32) #2

define dso_local void @test_cmul_s8_ld_incp_with_wrapper(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s8_ld_incp_with_wrapper:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    mv a3, a0
; ASM-NEXT:    esp.vld.128.ip q0, a3, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    li a1, 0
; ASM-NEXT:    esp.movx.w.sar a1
; ASM-NEXT:    esp.cmul.s8.ld.incp q1, a0, q0, q0, q1, 3
; ASM-NEXT:    esp.vst.128.ip q0, a2, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %cmul_ld_res = tail call { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s8.ld.incp.m(<16 x i8> undef, <16 x i8> %ev1, <16 x i8> %ev2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <16 x i8>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %ev3, ptr %dst, i32 16)
  ret void
}

declare { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s8.ld.incp.m(<16 x i8>, <16 x i8>, <16 x i8>, ptr, i32, i32) #2

define dso_local void @test_cmul_u16_st_incp(ptr noundef %src1, ptr noundef %src2, ptr noundef %src3, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_u16_st_incp:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    li a4, 0
; ASM-NEXT:    mv a5, a0
; ASM-NEXT:    esp.vld.128.ip q0, a5, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.movx.w.sar a4
; ASM-NEXT:    esp.cmul.u16.ld.incp q2, a0, q3, q0, q1, 3
; ASM-NEXT:    esp.vld.128.ip q2, a2, 16
; ASM-NEXT:    esp.cmul.u16.st.incp q2, a3, q3, q0, q1, 3
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %cmul_ld_res = tail call { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u16.ld.incp.m(<8 x i16> undef, <8 x i16> %bc1, <8 x i16> %bc2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <8 x i16>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vld3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src3, i32 16)
  %ev4 = extractvalue { <16 x i8>, ptr } %vld3, 0
  %cmul_st_res = tail call { <8 x i16>, ptr } @llvm.riscv.esp.cmul.u16.st.incp.m(<8 x i16> %ev3, <8 x i16> %bc1, <8 x i16> %bc2, <16 x i8> %ev4, ptr %dst, i32 3, i32 %sar)
  ret void
}

declare { <8 x i16>, ptr } @llvm.riscv.esp.cmul.u16.st.incp.m(<8 x i16>, <8 x i16>, <8 x i16>, <16 x i8>, ptr, i32, i32) #3

define dso_local void @test_cmul_s16_st_incp(ptr noundef %src1, ptr noundef %src2, ptr noundef %src3, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s16_st_incp:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    li a4, 0
; ASM-NEXT:    mv a5, a0
; ASM-NEXT:    esp.vld.128.ip q0, a5, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.movx.w.sar a4
; ASM-NEXT:    esp.cmul.s16.ld.incp q2, a0, q3, q0, q1, 3
; ASM-NEXT:    esp.vld.128.ip q2, a2, 16
; ASM-NEXT:    esp.cmul.s16.st.incp q2, a3, q3, q0, q1, 3
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %cmul_ld_res = tail call { <8 x i16>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s16.ld.incp.m(<8 x i16> undef, <8 x i16> %bc1, <8 x i16> %bc2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <8 x i16>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vld3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src3, i32 16)
  %ev4 = extractvalue { <16 x i8>, ptr } %vld3, 0
  %cmul_st_res = tail call { <8 x i16>, ptr } @llvm.riscv.esp.cmul.s16.st.incp.m(<8 x i16> %ev3, <8 x i16> %bc1, <8 x i16> %bc2, <16 x i8> %ev4, ptr %dst, i32 3, i32 %sar)
  ret void
}

declare { <8 x i16>, ptr } @llvm.riscv.esp.cmul.s16.st.incp.m(<8 x i16>, <8 x i16>, <8 x i16>, <16 x i8>, ptr, i32, i32) #3

define dso_local void @test_cmul_u8_st_incp(ptr noundef %src1, ptr noundef %src2, ptr noundef %src3, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_u8_st_incp:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    li a4, 0
; ASM-NEXT:    mv a5, a0
; ASM-NEXT:    esp.vld.128.ip q0, a5, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.movx.w.sar a4
; ASM-NEXT:    esp.cmul.u8.ld.incp q2, a0, q3, q0, q1, 3
; ASM-NEXT:    esp.vld.128.ip q2, a2, 16
; ASM-NEXT:    esp.cmul.u8.st.incp q2, a3, q3, q0, q1, 3
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %cmul_ld_res = tail call { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.u8.ld.incp.m(<16 x i8> undef, <16 x i8> %ev1, <16 x i8> %ev2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <16 x i8>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vld3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src3, i32 16)
  %ev4 = extractvalue { <16 x i8>, ptr } %vld3, 0
  %cmul_st_res = tail call { <16 x i8>, ptr } @llvm.riscv.esp.cmul.u8.st.incp.m(<16 x i8> %ev3, <16 x i8> %ev1, <16 x i8> %ev2, <16 x i8> %ev4, ptr %dst, i32 3, i32 %sar)
  ret void
}

declare { <16 x i8>, ptr } @llvm.riscv.esp.cmul.u8.st.incp.m(<16 x i8>, <16 x i8>, <16 x i8>, <16 x i8>, ptr, i32, i32) #3

define dso_local void @test_cmul_s8_st_incp(ptr noundef %src1, ptr noundef %src2, ptr noundef %src3, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s8_st_incp:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    li a4, 0
; ASM-NEXT:    mv a5, a0
; ASM-NEXT:    esp.vld.128.ip q0, a5, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.movx.w.sar a4
; ASM-NEXT:    esp.cmul.s8.ld.incp q2, a0, q3, q0, q1, 3
; ASM-NEXT:    esp.vld.128.ip q2, a2, 16
; ASM-NEXT:    esp.cmul.s8.st.incp q2, a3, q3, q0, q1, 3
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %cmul_ld_res = tail call { <16 x i8>, <16 x i8>, ptr } @llvm.riscv.esp.cmul.s8.ld.incp.m(<16 x i8> undef, <16 x i8> %ev1, <16 x i8> %ev2, ptr %src1, i32 3, i32 %sar)
  %ev3 = extractvalue { <16 x i8>, <16 x i8>, ptr } %cmul_ld_res, 0
  %vld3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src3, i32 16)
  %ev4 = extractvalue { <16 x i8>, ptr } %vld3, 0
  %cmul_st_res = tail call { <16 x i8>, ptr } @llvm.riscv.esp.cmul.s8.st.incp.m(<16 x i8> %ev3, <16 x i8> %ev1, <16 x i8> %ev2, <16 x i8> %ev4, ptr %dst, i32 3, i32 %sar)
  ret void
}

declare { <16 x i8>, ptr } @llvm.riscv.esp.cmul.s8.st.incp.m(<16 x i8>, <16 x i8>, <16 x i8>, <16 x i8>, ptr, i32, i32) #3

define dso_local void @test_cmul_s16_basic(ptr noundef %src1, ptr noundef %src2, ptr noundef %src3, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s16_basic:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    esp.vld.128.ip q2, a2, 16
; ASM-NEXT:    li a0, 0
; ASM-NEXT:    esp.movx.w.sar a0
; ASM-NEXT:    esp.cmul.s16 q2, q0, q1, 0
; ASM-NEXT:    esp.vst.128.ip q2, a3, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %vld3 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src3, i32 16)
  %ev3 = extractvalue { <16 x i8>, ptr } %vld3, 0
  %bc3 = bitcast <16 x i8> %ev3 to <8 x i16>
  %v1 = tail call <8 x i16> @llvm.riscv.esp.cmul.s16.m(<8 x i16> %bc3, <8 x i16> %bc1, <8 x i16> %bc2, i32 0, i32 %sar)
  %bc4 = bitcast <8 x i16> %v1 to <16 x i8>
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %bc4, ptr %dst, i32 16)
  ret void
}

declare <8 x i16> @llvm.riscv.esp.cmul.s16.m(<8 x i16>, <8 x i16>, <8 x i16>, i32 immarg, i32) #1

define dso_local void @test_cmul_s16_rmw(ptr noundef %src1, ptr noundef %src2, ptr noundef %dst) local_unnamed_addr #0 {
; ASM-LABEL: test_cmul_s16_rmw:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.vld.128.ip q0, a0, 16
; ASM-NEXT:    esp.vld.128.ip q1, a1, 16
; ASM-NEXT:    li a0, 0
; ASM-NEXT:    esp.movx.w.sar a0
; ASM-NEXT:    esp.cmul.s16 q2, q0, q1, 0
; ASM-NEXT:    esp.cmul.s16 q2, q0, q1, 1
; ASM-NEXT:    esp.vst.128.ip q2, a2, 16
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 0)
  %vld1 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src1, i32 16)
  %ev1 = extractvalue { <16 x i8>, ptr } %vld1, 0
  %bc1 = bitcast <16 x i8> %ev1 to <8 x i16>
  %vld2 = tail call { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr %src2, i32 16)
  %ev2 = extractvalue { <16 x i8>, ptr } %vld2, 0
  %bc2 = bitcast <16 x i8> %ev2 to <8 x i16>
  %v1 = tail call <8 x i16> @llvm.riscv.esp.cmul.s16.m(<8 x i16> undef, <8 x i16> %bc1, <8 x i16> %bc2, i32 0, i32 %sar)
  %v2 = tail call <8 x i16> @llvm.riscv.esp.cmul.s16.m(<8 x i16> %v1, <8 x i16> %bc1, <8 x i16> %bc2, i32 1, i32 %sar)
  %bc3 = bitcast <8 x i16> %v2 to <16 x i8>
  %vst_ptr = tail call ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8> %bc3, ptr %dst, i32 16)
  ret void
}

define dso_local i32 @test_movx_r_sar(i32 noundef %rs1_val) local_unnamed_addr #4 {
; ASM-LABEL: test_movx_r_sar:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.movx.w.sar a0
; ASM-NEXT:    esp.movx.r.sar a0
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 %rs1_val)
  %v1 = tail call i32 @llvm.riscv.esp.movx.r.sar.m(i32 %sar)
  ret i32 %v1
}

declare i32 @llvm.riscv.esp.movx.r.sar.m(i32) #1

define dso_local i32 @test_movx_sar_write_read(i32 noundef %rs1_val) local_unnamed_addr #4 {
; ASM-LABEL: test_movx_sar_write_read:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    esp.movx.w.sar a0
; ASM-NEXT:    esp.movx.r.sar a0
; ASM-NEXT:    ret
entry:
  %sar = tail call i32 @llvm.riscv.esp.movx.w.sar.m(i32 %rs1_val)
  %v1 = tail call i32 @llvm.riscv.esp.movx.r.sar.m(i32 %sar)
  ret i32 %v1
}

declare { <16 x i8>, ptr } @llvm.riscv.esp.vld.128.ip.m(ptr, i32) #2

declare ptr @llvm.riscv.esp.vst.128.ip.m(<16 x i8>, ptr, i32) #3

attributes #0 = { "target-features"="+32bit,+xespv" }
attributes #1 = { nounwind }
attributes #2 = { nounwind }
attributes #3 = { nounwind }
attributes #4 = { "target-features"="+32bit,+xespv" }


