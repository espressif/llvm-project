// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 5
// RUN: %clang_cc1 -triple riscv32 -target-feature +xespdsp -emit-llvm -O0 -o - %s \
// RUN: | FileCheck %s

#include <stdint.h>

// CHECK-LABEL: define dso_local void @test(
// CHECK-SAME: ) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[DATA:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 10, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.muls16ix2(i32 [[TMP0]], i32 [[TMP1]], i32 29, i32 [[TMP2]], i32 [[TMP3]])
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.muls16x2(i32 [[TMP4]], i32 [[TMP5]], i32 [[TMP6]], i32 [[TMP7]])
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = call i32 @llvm.riscv.esp.muls32i(i32 [[TMP8]], i32 [[TMP9]], i32 3)
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = call i32 @llvm.riscv.esp.muls32(i32 [[TMP11]], i32 [[TMP12]])
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = call i32 @llvm.riscv.esp.shl(i32 [[TMP14]], i32 [[TMP15]])
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = call i32 @llvm.riscv.esp.shr(i32 [[TMP17]], i32 [[TMP18]])
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = call i32 @llvm.riscv.esp.abs(i32 [[TMP20]])
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = call i32 @llvm.riscv.esp.movr.xacc(i32 [[TMP22]])
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = call i32 @llvm.riscv.esp.min(i32 [[TMP24]], i32 [[TMP25]])
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = call i32 @llvm.riscv.esp.max(i32 [[TMP27]], i32 [[TMP28]])
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP31:%.*]] = call i32 @llvm.riscv.esp.clzs(i32 [[TMP30]])
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = call float @llvm.riscv.esp.flw.ip(i32 [[TMP32]], i32 -5, i32 2)
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP36:%.*]] = call float @llvm.riscv.esp.flw.xp(i32 [[TMP34]], i32 [[TMP35]])
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast i32 [[TMP38]] to float
// CHECK-NEXT:    call void @llvm.riscv.esp.fsw.ip(i32 [[TMP37]], float [[TMP39]], i32 4, i32 3)
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast i32 [[TMP42]] to float
// CHECK-NEXT:    call void @llvm.riscv.esp.fsw.xp(i32 [[TMP40]], i32 [[TMP41]], float [[TMP43]])
// CHECK-NEXT:    [[TMP44:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP45:%.*]] = call i32 @llvm.riscv.esp.lw.ip(i32 [[TMP44]], i32 -8, i32 1)
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP48:%.*]] = call i32 @llvm.riscv.esp.lw.xp(i32 [[TMP46]], i32 [[TMP47]])
// CHECK-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP50:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sw.ip(i32 [[TMP49]], i32 [[TMP50]], i32 -10, i32 0)
// CHECK-NEXT:    [[TMP51:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP52:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP53:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sw.xp(i32 [[TMP51]], i32 [[TMP52]], i32 [[TMP53]])
// CHECK-NEXT:    [[TMP54:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP55:%.*]] = call i32 @llvm.riscv.esp.lh.ip(i32 [[TMP54]], i32 3, i32 1)
// CHECK-NEXT:    [[TMP56:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP57:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP58:%.*]] = call i32 @llvm.riscv.esp.lh.xp(i32 [[TMP56]], i32 [[TMP57]])
// CHECK-NEXT:    [[TMP59:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP60:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sh.ip(i32 [[TMP59]], i32 [[TMP60]], i32 14, i32 0)
// CHECK-NEXT:    [[TMP61:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP62:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP63:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sh.xp(i32 [[TMP61]], i32 [[TMP62]], i32 [[TMP63]])
// CHECK-NEXT:    [[TMP64:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP65:%.*]] = call i32 @llvm.riscv.esp.lb.ip(i32 [[TMP64]], i32 11, i32 0)
// CHECK-NEXT:    [[TMP66:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP67:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP68:%.*]] = call i32 @llvm.riscv.esp.lb.xp(i32 [[TMP66]], i32 [[TMP67]])
// CHECK-NEXT:    [[TMP69:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP70:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sb.ip(i32 [[TMP69]], i32 [[TMP70]], i32 7, i32 2)
// CHECK-NEXT:    [[TMP71:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP72:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP73:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.sb.xp(i32 [[TMP71]], i32 [[TMP72]], i32 [[TMP73]])
// CHECK-NEXT:    [[TMP74:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP75:%.*]] = call i32 @llvm.riscv.esp.lhu.ip(i32 [[TMP74]], i32 -12, i32 2)
// CHECK-NEXT:    [[TMP76:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP77:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP78:%.*]] = call i32 @llvm.riscv.esp.lhu.xp(i32 [[TMP76]], i32 [[TMP77]])
// CHECK-NEXT:    [[TMP79:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP80:%.*]] = call i32 @llvm.riscv.esp.lbu.ip(i32 [[TMP79]], i32 12, i32 2)
// CHECK-NEXT:    [[TMP81:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP82:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP83:%.*]] = call i32 @llvm.riscv.esp.lbu.xp(i32 [[TMP81]], i32 [[TMP82]])
// CHECK-NEXT:    [[TMP84:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP85:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.movw.xacc(i32 [[TMP84]], i32 [[TMP85]])
// CHECK-NEXT:    [[TMP86:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP87:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs32i(i32 [[TMP86]], i32 [[TMP87]], i32 18)
// CHECK-NEXT:    [[TMP88:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP89:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs32(i32 [[TMP88]], i32 [[TMP89]])
// CHECK-NEXT:    [[TMP90:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP91:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP92:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP93:%.*]] = call i32 @llvm.riscv.esp.macs32.ld(i32 [[TMP90]], i32 [[TMP91]], i32 [[TMP92]])
// CHECK-NEXT:    [[TMP94:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP95:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP96:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP97:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs32.st(i32 [[TMP94]], i32 [[TMP95]], i32 [[TMP96]], i32 [[TMP97]])
// CHECK-NEXT:    [[TMP98:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP99:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs16ix2(i32 [[TMP98]], i32 [[TMP99]], i32 25)
// CHECK-NEXT:    [[TMP100:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP101:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs16x2(i32 [[TMP100]], i32 [[TMP101]])
// CHECK-NEXT:    [[TMP102:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP103:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP104:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP105:%.*]] = call i32 @llvm.riscv.esp.macs16x2.ld(i32 [[TMP102]], i32 [[TMP103]], i32 [[TMP104]])
// CHECK-NEXT:    [[TMP106:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP107:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP108:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP109:%.*]] = call i32 @llvm.riscv.esp.macs16x1.ld(i32 [[TMP106]], i32 [[TMP107]], i32 [[TMP108]])
// CHECK-NEXT:    [[TMP110:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP111:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP112:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP113:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.macs16x2.st(i32 [[TMP110]], i32 [[TMP111]], i32 [[TMP112]], i32 [[TMP113]])
// CHECK-NEXT:    [[TMP114:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP115:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus32i(i32 [[TMP114]], i32 [[TMP115]], i32 3)
// CHECK-NEXT:    [[TMP116:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP117:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus32(i32 [[TMP116]], i32 [[TMP117]])
// CHECK-NEXT:    [[TMP118:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP119:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP120:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP121:%.*]] = call i32 @llvm.riscv.esp.msus32.ld(i32 [[TMP118]], i32 [[TMP119]], i32 [[TMP120]])
// CHECK-NEXT:    [[TMP122:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP123:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP124:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP125:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus32.st(i32 [[TMP122]], i32 [[TMP123]], i32 [[TMP124]], i32 [[TMP125]])
// CHECK-NEXT:    [[TMP126:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP127:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus16ix2(i32 [[TMP126]], i32 [[TMP127]], i32 23)
// CHECK-NEXT:    [[TMP128:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP129:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus16x2(i32 [[TMP128]], i32 [[TMP129]])
// CHECK-NEXT:    [[TMP130:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP131:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP132:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP133:%.*]] = call i32 @llvm.riscv.esp.msus16x2.ld(i32 [[TMP130]], i32 [[TMP131]], i32 [[TMP132]])
// CHECK-NEXT:    [[TMP134:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP135:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP136:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP137:%.*]] = call i32 @llvm.riscv.esp.msus16x1.ld(i32 [[TMP134]], i32 [[TMP135]], i32 [[TMP136]])
// CHECK-NEXT:    [[TMP138:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP139:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP140:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    [[TMP141:%.*]] = load i32, ptr [[DATA]], align 4
// CHECK-NEXT:    call void @llvm.riscv.esp.msus16x2.st(i32 [[TMP138]], i32 [[TMP139]], i32 [[TMP140]], i32 [[TMP141]])
// CHECK-NEXT:    ret void
//
void test() {
    uint32_t data = 10;
    __builtin_riscv_esp_muls16ix2(data, data, 29, data, data);
    __builtin_riscv_esp_muls16x2(data, data, data, data);
    __builtin_riscv_esp_muls32i(data, data, 3);
    __builtin_riscv_esp_muls32(data, data);
    __builtin_riscv_esp_shl(data, data);
    __builtin_riscv_esp_shr(data, data);
    __builtin_riscv_esp_abs(data);
    __builtin_riscv_esp_movr_xacc(data);
    __builtin_riscv_esp_min(data, data);
    __builtin_riscv_esp_max(data, data);
    __builtin_riscv_esp_clzs(data);
    __builtin_riscv_esp_flw_ip(data, -5, 2);
    __builtin_riscv_esp_flw_xp(data, data);
    __builtin_riscv_esp_fsw_ip(data, data, 4, 3);
    __builtin_riscv_esp_fsw_xp(data, data, data);
    __builtin_riscv_esp_lw_ip(data, -8, 1);
    __builtin_riscv_esp_lw_xp(data, data);
    __builtin_riscv_esp_sw_ip(data, data, -10, 0);
    __builtin_riscv_esp_sw_xp(data, data, data);
    __builtin_riscv_esp_lh_ip(data, 3, 1);
    __builtin_riscv_esp_lh_xp(data, data);
    __builtin_riscv_esp_sh_ip(data, data, 14, 0);
    __builtin_riscv_esp_sh_xp(data, data, data);
    __builtin_riscv_esp_lb_ip(data, 11, 0);
    __builtin_riscv_esp_lb_xp(data, data);
    __builtin_riscv_esp_sb_ip(data, data, 7, 2);
    __builtin_riscv_esp_sb_xp(data, data, data);
    __builtin_riscv_esp_lhu_ip(data, -12, 2);
    __builtin_riscv_esp_lhu_xp(data, data);
    __builtin_riscv_esp_lbu_ip(data, 12, 2);
    __builtin_riscv_esp_lbu_xp(data, data);
    __builtin_riscv_esp_movw_xacc(data, data);
    __builtin_riscv_esp_macs32i(data, data, 18);
    __builtin_riscv_esp_macs32(data, data);
    __builtin_riscv_esp_macs32_ld(data, data, data);
    __builtin_riscv_esp_macs32_st(data, data, data, data);
    __builtin_riscv_esp_macs16ix2(data, data, 25);
    __builtin_riscv_esp_macs16x2(data, data);
    __builtin_riscv_esp_macs16x2_ld(data, data, data);
    __builtin_riscv_esp_macs16x1_ld(data, data, data);
    __builtin_riscv_esp_macs16x2_st(data, data, data, data);
    __builtin_riscv_esp_msus32i(data, data, 3);
    __builtin_riscv_esp_msus32(data, data);
    __builtin_riscv_esp_msus32_ld(data, data, data);
    __builtin_riscv_esp_msus32_st(data, data, data, data);
    __builtin_riscv_esp_msus16ix2(data, data, 23);
    __builtin_riscv_esp_msus16x2(data, data);
    __builtin_riscv_esp_msus16x2_ld(data, data, data);
    __builtin_riscv_esp_msus16x1_ld(data, data, data);
    __builtin_riscv_esp_msus16x2_st(data, data, data, data);
}
